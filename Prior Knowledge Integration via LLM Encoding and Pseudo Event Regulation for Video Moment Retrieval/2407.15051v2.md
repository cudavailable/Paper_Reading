## 标题：
Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation for Video Moment Retrieval
基于LLM编码和伪事件调节的视频时刻检索的先验知识集成

## 关键词：
Video Moment Retrieval, Highlight Detection, LLMs

## 术语：

### Video moment retrieval：
	从大规模视频数据中检索出用户感兴趣的特定视频片段或时刻的过程。这个过程通常涉及使用文本查询、视频内容特征或其他元数据来定位和提取视频中与查询相关的片段。


## 内容：

### 1 INTRODUCTION：

**后GPT时代的有力工具**：LLM，擅长captioning & grounding。

**挑战**：
1. LLM更擅长捕捉高层级的语义，而难以检测帧间细粒度的变化。
2. 要用Q-Former将视觉表征转化成LLM可用的token，但是Q-Former和LLM都未在帧级别训练过。
3. LLM decoder输出的文本token是离散的确定的，难以兼容连续的可比的显著性分数。

**解决方案**：
- 使用==LLM encoder==而不是decoder。
- 通过事件检测技术引入==伪事件==，使预测的时刻与事件保持一致。

### 2 RELATED WORK：

#### 2.1 Moment Retrieval and Highlight Detection：

**MR传统方法**：
- Proposal based
- Proposal-free

**HD核心**：
评分每个视频剪辑的重要性，无论是仅基于视觉还是基于视觉-音频组合输入。

#### 2.2 Vision-Language Models：

*这部分介绍了若干可作为decoder的语言模型，最终整体构成视觉语言模型。*

目前将LLM作为语义关系细化encoder的思路，仅在一篇文献中体现。但是本篇文章与之存在如下**区别**：
- 进行了一项可行性研究来调查这种方法背后的基本原理。
- 关注于LLM中先验知识的集成。

### 3 LEVERAGING LLM ENCODERS FOR INTER-CONCEPT RELATION REFINEMENT：

*这部分将完成可行性研究，即调查该方法是否适用于多模态嵌入并了解背后的原因。另外还探索了是否有降低计算开销的可能。*

#### 3.1 LLM Encoders are Relation Refiners：

*We conducted an experiment to validate the hypothesis that an LLM Encoder can be utilized for relation refinement.*

**实验结果**：
- 原本unreasonable的组合，经过LLM encoder (LLaMA-2 encoder)后，有83.33%被转化为reasonable。而原来已reasonable的组合没有被错误转化。

#### 3.2 LLM Encoders for Multimodal Embeddings：

*While the LLM encoder’s capability to refine semantic relations is expected, our primary focus lies in investigating whether this ability can be transferred to multimodal embeddings.*

###### Dose the LLM encoders work for fused embeddings?

	$c ∗ = (1 − 𝛼)c + 𝛼c ′ .$
	其中c是文本嵌入，而c'是非文本嵌入。

- 可以看到，参数alpha决定了文本嵌入受非文本嵌入（随机性和错位）的影响程度。
- 结果是，当alpha较小时，文本嵌入占主导地位，LLaMA encoder有效地将多数不合理的组合正确修改了。

###### What will happen when non-textual embeddings are distorted?

**实验**：通过将非文本嵌入向量c‘的元素按照概率p随机设置为零来引入失真因子，从而产生失真向量c’𝑝。然后用c’𝑝替换公式中的c‘。

**结果**：当文本嵌入占主导地位时，非文本嵌入的失真不会退化改善效果。可以确信LLM encoder作为可靠的relation refiner的有效性。

**Rationale**：LLM encoder -> Transfomer arch. -> self-attention -> similarity

#### 3.3 Using A Subset of Layers as the Encoder：

*Our last experiment for the feasibility is to use a subset of LLaMA encoder as a relation refiner.*

**好处**：若验证了LLM的子层工作的有效性，则可节省计算资源，扩展应用。

*The results in Fig. 3 validate the hypothesis by demonstrating that a subset of the LLaMA encoder can also effectively refine relations.*

### 4 METHOD：

#### 4.1 A General Framework for VMR：

*这部分讲述了VMR架构的流程。*

- **Input** :  a video 𝑉 and a textual query 𝑄.
- **output** : a set of candidate video segments {m𝑘 } that are relevant to the query 𝑄.

	${m𝑘 } = 𝑓𝜃 (𝑄,𝑉 ).$

- This process is accomplished by an inference process of a neural network 𝑓𝜃 with parameters 𝜃.

#### 4.2 Integration of Semantic Refinement：

*To incorporate ==semantic refinement== within the general framework, we can insert an ==LLM encoder== between the fusion and decoding processes.*

- 为了使模型对整个前景和背景有更好的理解，且不被主导的前景概念误导，所以依据前述可行性研究，将引入LLM encoder。
- 为减少计算资源开销，使用LLM的子层作为encoder。
- 为适应LLM encoder的输入维度，需要在LLM encoder前后各设一线性层。

#### 4.3 Integration of Pseudo-Event Regulation：

*The motivation for this regulation arises from the recognition that valid moments should remain within the boundaries of events instead of crossing them.*

1. 使用event detector为给定的视频生成伪事件。
2. 计算伪事件调节损失$L𝑒𝑣𝑡$。
3. 计算同一事件的成员帧位置嵌入矩阵与中心帧位置嵌入向量之间的差损失$L𝑝𝑜𝑠$。
4. 计算总的损失$L = L𝑚𝑛𝑡 + 𝜆𝑒L𝑒𝑣𝑡 + 𝜆𝑝L𝑝𝑜𝑠$。
5. 优化参数𝜃 = {𝜃𝐹 , 𝜃𝐷, 𝜃𝐿1, 𝜃𝐿2}。

### 5 EXPERIMENTS：

#### 5.1 Dataset and Evaluation Metrics：

1) *Joint moment retrieval and highlight detection* on QVHighlights, which includes more than 10,000 videos with high-quality text queries.
2) *Individual moment retrieval* on Charades-STA and TACoS.
3) *Highlight detection* on TVSum and Youtube-HL.

#### 5.2 Implementation Details:

1) SlowFast and CLIP models to extract video features on QVHighlights dataset every 2 seconds.
2) Slowfast+Clip and VGG on Charades-STA and TACoS datasets, where video features are extracted every 1 second and 2 seconds.
3) Extract clip-level features by a pre-trained I3D.

**确定模型配置**：LLM使用子层，encoder、decoder使用6层Transformer，确定超参数lambda、优化器、学习率、batch_size、epoch_num等。

#### 5.3 Comparison with State-of-the-arts：

*The consistent outperformance across various benchmarks and comparison with a wide range of SOTA methods suggest the effectiveness of incorporating prior knowledge from large language models.*

#### 5.4 Ablation study：

1. Effectiveness of using LLM encoders as relation refiners.
2. Effectiveness of the pseudo-event regulation.
3. Effectiveness of position embedding regulation.
4. Study of the compatibility to different frameworks.

### 6 CONCLUSION
