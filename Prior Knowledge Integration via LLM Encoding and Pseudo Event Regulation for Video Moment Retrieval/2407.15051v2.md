## æ ‡é¢˜ï¼š
Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation for Video Moment Retrieval
åŸºäºLLMç¼–ç å’Œä¼ªäº‹ä»¶è°ƒèŠ‚çš„è§†é¢‘æ—¶åˆ»æ£€ç´¢çš„å…ˆéªŒçŸ¥è¯†é›†æˆ

## å…³é”®è¯ï¼š
Video Moment Retrieval, Highlight Detection, LLMs

## æœ¯è¯­ï¼š

### Video moment retrievalï¼š
	ä»å¤§è§„æ¨¡è§†é¢‘æ•°æ®ä¸­æ£€ç´¢å‡ºç”¨æˆ·æ„Ÿå…´è¶£çš„ç‰¹å®šè§†é¢‘ç‰‡æ®µæˆ–æ—¶åˆ»çš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸æ¶‰åŠä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢ã€è§†é¢‘å†…å®¹ç‰¹å¾æˆ–å…¶ä»–å…ƒæ•°æ®æ¥å®šä½å’Œæå–è§†é¢‘ä¸­ä¸æŸ¥è¯¢ç›¸å…³çš„ç‰‡æ®µã€‚


## å†…å®¹ï¼š

### 1 INTRODUCTIONï¼š

**åGPTæ—¶ä»£çš„æœ‰åŠ›å·¥å…·**ï¼šLLMï¼Œæ“…é•¿captioning & groundingã€‚

**æŒ‘æˆ˜**ï¼š
1. LLMæ›´æ“…é•¿æ•æ‰é«˜å±‚çº§çš„è¯­ä¹‰ï¼Œè€Œéš¾ä»¥æ£€æµ‹å¸§é—´ç»†ç²’åº¦çš„å˜åŒ–ã€‚
2. è¦ç”¨Q-Formerå°†è§†è§‰è¡¨å¾è½¬åŒ–æˆLLMå¯ç”¨çš„tokenï¼Œä½†æ˜¯Q-Formerå’ŒLLMéƒ½æœªåœ¨å¸§çº§åˆ«è®­ç»ƒè¿‡ã€‚
3. LLM decoderè¾“å‡ºçš„æ–‡æœ¬tokenæ˜¯ç¦»æ•£çš„ç¡®å®šçš„ï¼Œéš¾ä»¥å…¼å®¹è¿ç»­çš„å¯æ¯”çš„æ˜¾è‘—æ€§åˆ†æ•°ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨==LLM encoder==è€Œä¸æ˜¯decoderã€‚
- é€šè¿‡äº‹ä»¶æ£€æµ‹æŠ€æœ¯å¼•å…¥==ä¼ªäº‹ä»¶==ï¼Œä½¿é¢„æµ‹çš„æ—¶åˆ»ä¸äº‹ä»¶ä¿æŒä¸€è‡´ã€‚

### 2 RELATED WORKï¼š

#### 2.1 Moment Retrieval and Highlight Detectionï¼š

**MRä¼ ç»Ÿæ–¹æ³•**ï¼š
- Proposal based
- Proposal-free

**HDæ ¸å¿ƒ**ï¼š
è¯„åˆ†æ¯ä¸ªè§†é¢‘å‰ªè¾‘çš„é‡è¦æ€§ï¼Œæ— è®ºæ˜¯ä»…åŸºäºè§†è§‰è¿˜æ˜¯åŸºäºè§†è§‰-éŸ³é¢‘ç»„åˆè¾“å…¥ã€‚

#### 2.2 Vision-Language Modelsï¼š

*è¿™éƒ¨åˆ†ä»‹ç»äº†è‹¥å¹²å¯ä½œä¸ºdecoderçš„è¯­è¨€æ¨¡å‹ï¼Œæœ€ç»ˆæ•´ä½“æ„æˆè§†è§‰è¯­è¨€æ¨¡å‹ã€‚*

ç›®å‰å°†LLMä½œä¸ºè¯­ä¹‰å…³ç³»ç»†åŒ–encoderçš„æ€è·¯ï¼Œä»…åœ¨ä¸€ç¯‡æ–‡çŒ®ä¸­ä½“ç°ã€‚ä½†æ˜¯æœ¬ç¯‡æ–‡ç« ä¸ä¹‹å­˜åœ¨å¦‚ä¸‹**åŒºåˆ«**ï¼š
- è¿›è¡Œäº†ä¸€é¡¹å¯è¡Œæ€§ç ”ç©¶æ¥è°ƒæŸ¥è¿™ç§æ–¹æ³•èƒŒåçš„åŸºæœ¬åŸç†ã€‚
- å…³æ³¨äºLLMä¸­å…ˆéªŒçŸ¥è¯†çš„é›†æˆã€‚

### 3 LEVERAGING LLM ENCODERS FOR INTER-CONCEPT RELATION REFINEMENTï¼š

*è¿™éƒ¨åˆ†å°†å®Œæˆå¯è¡Œæ€§ç ”ç©¶ï¼Œå³è°ƒæŸ¥è¯¥æ–¹æ³•æ˜¯å¦é€‚ç”¨äºå¤šæ¨¡æ€åµŒå…¥å¹¶äº†è§£èƒŒåçš„åŸå› ã€‚å¦å¤–è¿˜æ¢ç´¢äº†æ˜¯å¦æœ‰é™ä½è®¡ç®—å¼€é”€çš„å¯èƒ½ã€‚*

#### 3.1 LLM Encoders are Relation Refinersï¼š

*We conducted an experiment to validate the hypothesis that an LLM Encoder can be utilized for relation refinement.*

**å®éªŒç»“æœ**ï¼š
- åŸæœ¬unreasonableçš„ç»„åˆï¼Œç»è¿‡LLM encoder (LLaMA-2 encoder)åï¼Œæœ‰83.33%è¢«è½¬åŒ–ä¸ºreasonableã€‚è€ŒåŸæ¥å·²reasonableçš„ç»„åˆæ²¡æœ‰è¢«é”™è¯¯è½¬åŒ–ã€‚

#### 3.2 LLM Encoders for Multimodal Embeddingsï¼š

*While the LLM encoderâ€™s capability to refine semantic relations is expected, our primary focus lies in investigating whether this ability can be transferred to multimodal embeddings.*

###### Dose the LLM encoders work for fused embeddings?

	$c âˆ— = (1 âˆ’ ğ›¼)c + ğ›¼c â€² .$
	å…¶ä¸­cæ˜¯æ–‡æœ¬åµŒå…¥ï¼Œè€Œc'æ˜¯éæ–‡æœ¬åµŒå…¥ã€‚

- å¯ä»¥çœ‹åˆ°ï¼Œå‚æ•°alphaå†³å®šäº†æ–‡æœ¬åµŒå…¥å—éæ–‡æœ¬åµŒå…¥ï¼ˆéšæœºæ€§å’Œé”™ä½ï¼‰çš„å½±å“ç¨‹åº¦ã€‚
- ç»“æœæ˜¯ï¼Œå½“alphaè¾ƒå°æ—¶ï¼Œæ–‡æœ¬åµŒå…¥å ä¸»å¯¼åœ°ä½ï¼ŒLLaMA encoderæœ‰æ•ˆåœ°å°†å¤šæ•°ä¸åˆç†çš„ç»„åˆæ­£ç¡®ä¿®æ”¹äº†ã€‚

###### What will happen when non-textual embeddings are distorted?

**å®éªŒ**ï¼šé€šè¿‡å°†éæ–‡æœ¬åµŒå…¥å‘é‡câ€˜çš„å…ƒç´ æŒ‰ç…§æ¦‚ç‡péšæœºè®¾ç½®ä¸ºé›¶æ¥å¼•å…¥å¤±çœŸå› å­ï¼Œä»è€Œäº§ç”Ÿå¤±çœŸå‘é‡câ€™ğ‘ã€‚ç„¶åç”¨câ€™ğ‘æ›¿æ¢å…¬å¼ä¸­çš„câ€˜ã€‚

**ç»“æœ**ï¼šå½“æ–‡æœ¬åµŒå…¥å ä¸»å¯¼åœ°ä½æ—¶ï¼Œéæ–‡æœ¬åµŒå…¥çš„å¤±çœŸä¸ä¼šé€€åŒ–æ”¹å–„æ•ˆæœã€‚å¯ä»¥ç¡®ä¿¡LLM encoderä½œä¸ºå¯é çš„relation refinerçš„æœ‰æ•ˆæ€§ã€‚

**Rationale**ï¼šLLM encoder -> Transfomer arch. -> self-attention -> similarity

#### 3.3 Using A Subset of Layers as the Encoderï¼š

*Our last experiment for the feasibility is to use a subset of LLaMA encoder as a relation refiner.*

**å¥½å¤„**ï¼šè‹¥éªŒè¯äº†LLMçš„å­å±‚å·¥ä½œçš„æœ‰æ•ˆæ€§ï¼Œåˆ™å¯èŠ‚çœè®¡ç®—èµ„æºï¼Œæ‰©å±•åº”ç”¨ã€‚

*The results in Fig. 3 validate the hypothesis by demonstrating that a subset of the LLaMA encoder can also effectively refine relations.*

### 4 METHODï¼š

#### 4.1 A General Framework for VMRï¼š

*è¿™éƒ¨åˆ†è®²è¿°äº†VMRæ¶æ„çš„æµç¨‹ã€‚*

- **Input** :  a video ğ‘‰ and a textual query ğ‘„.
- **output** : a set of candidate video segments {mğ‘˜ } that are relevant to the query ğ‘„.

	${mğ‘˜ } = ğ‘“ğœƒ (ğ‘„,ğ‘‰ ).$

- This process is accomplished by an inference process of a neural network ğ‘“ğœƒ with parameters ğœƒ.

#### 4.2 Integration of Semantic Refinementï¼š

*To incorporate ==semantic refinement== within the general framework, we can insert an ==LLM encoder== between the fusion and decoding processes.*

- ä¸ºäº†ä½¿æ¨¡å‹å¯¹æ•´ä¸ªå‰æ™¯å’ŒèƒŒæ™¯æœ‰æ›´å¥½çš„ç†è§£ï¼Œä¸”ä¸è¢«ä¸»å¯¼çš„å‰æ™¯æ¦‚å¿µè¯¯å¯¼ï¼Œæ‰€ä»¥ä¾æ®å‰è¿°å¯è¡Œæ€§ç ”ç©¶ï¼Œå°†å¼•å…¥LLM encoderã€‚
- ä¸ºå‡å°‘è®¡ç®—èµ„æºå¼€é”€ï¼Œä½¿ç”¨LLMçš„å­å±‚ä½œä¸ºencoderã€‚
- ä¸ºé€‚åº”LLM encoderçš„è¾“å…¥ç»´åº¦ï¼Œéœ€è¦åœ¨LLM encoderå‰åå„è®¾ä¸€çº¿æ€§å±‚ã€‚

#### 4.3 Integration of Pseudo-Event Regulationï¼š

*The motivation for this regulation arises from the recognition that valid moments should remain within the boundaries of events instead of crossing them.*

1. ä½¿ç”¨event detectorä¸ºç»™å®šçš„è§†é¢‘ç”Ÿæˆä¼ªäº‹ä»¶ã€‚
2. è®¡ç®—ä¼ªäº‹ä»¶è°ƒèŠ‚æŸå¤±$Lğ‘’ğ‘£ğ‘¡$ã€‚
3. è®¡ç®—åŒä¸€äº‹ä»¶çš„æˆå‘˜å¸§ä½ç½®åµŒå…¥çŸ©é˜µä¸ä¸­å¿ƒå¸§ä½ç½®åµŒå…¥å‘é‡ä¹‹é—´çš„å·®æŸå¤±$Lğ‘ğ‘œğ‘ $ã€‚
4. è®¡ç®—æ€»çš„æŸå¤±$L = Lğ‘šğ‘›ğ‘¡ + ğœ†ğ‘’Lğ‘’ğ‘£ğ‘¡ + ğœ†ğ‘Lğ‘ğ‘œğ‘ $ã€‚
5. ä¼˜åŒ–å‚æ•°ğœƒ = {ğœƒğ¹ , ğœƒğ·, ğœƒğ¿1, ğœƒğ¿2}ã€‚

### 5 EXPERIMENTSï¼š

#### 5.1 Dataset and Evaluation Metricsï¼š

1) *Joint moment retrieval and highlight detection* on QVHighlights, which includes more than 10,000 videos with high-quality text queries.
2) *Individual moment retrieval* on Charades-STA and TACoS.
3) *Highlight detection* on TVSum and Youtube-HL.

#### 5.2 Implementation Details:

1) SlowFast and CLIP models to extract video features on QVHighlights dataset every 2 seconds.
2) Slowfast+Clip and VGG on Charades-STA and TACoS datasets, where video features are extracted every 1 second and 2 seconds.
3) Extract clip-level features by a pre-trained I3D.

**ç¡®å®šæ¨¡å‹é…ç½®**ï¼šLLMä½¿ç”¨å­å±‚ï¼Œencoderã€decoderä½¿ç”¨6å±‚Transformerï¼Œç¡®å®šè¶…å‚æ•°lambdaã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡ã€batch_sizeã€epoch_numç­‰ã€‚

#### 5.3 Comparison with State-of-the-artsï¼š

*The consistent outperformance across various benchmarks and comparison with a wide range of SOTA methods suggest the effectiveness of incorporating prior knowledge from large language models.*

#### 5.4 Ablation studyï¼š

1. Effectiveness of using LLM encoders as relation refiners.
2. Effectiveness of the pseudo-event regulation.
3. Effectiveness of position embedding regulation.
4. Study of the compatibility to different frameworks.

### 6 CONCLUSION
