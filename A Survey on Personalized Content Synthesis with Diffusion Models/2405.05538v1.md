## 标题：
- A Survey on Personalized Content Synthesis with Diffusion Models
- 基于扩散模型的个性化内容合成研究综述

## 关键词：
Generative Models, Diffusion Models, Personalized Content Synthesis

## 术语：

### Personalized Content Synthesis：
	通常指的是使用技术手段根据用户的偏好、兴趣和需求，生成定制化的内容。这种技术可以应用于多个领域，如个性化推荐系统、个性化广告、个性化新闻推送等。

### Generative Models：
	是一类机器学习模型，旨在从训练数据中学习数据的分布，从而能够生成类似于训练数据的新数据样本。这些模型旨在捕捉数据背后的潜在结构和规律，使其能够生成具有相似特征的全新数据。

### Diffusion Models：
	是一种用于建模和处理数据的概率生成模型。这一模型的主要思想是通过一系列扩散步骤来生成数据，每一步都使数据逐渐随机漂移，最终生成具有所需分布的数据样本。其能够灵活地控制生成过程中的噪声水平，从而实现对生成样本的精细调控。

### Subject of Interest：
	通常指的是在特定上下文中对某个主题、话题或对象表现出浓厚兴趣的个人或群体。这个术语通常在研究、调查、市场营销和个性化推荐等领域中使用。

### Image Alignment：
	指将两幅或多幅图像进行准确对齐的过程。在图像处理和计算机视觉领域，图像对准是一个重要的任务，用于将不同图像中的相似或相关内容进行对齐，以便进行比较、融合或分析。

### Text Fidelity：
	指的是文本生成或转换模型生成的文本与原始文本之间的相似程度或准确性。在自然语言处理领域中，文本保真度是评估模型生成文本质量的重要指标之一。

### DDPMs(Denoising Diffusion Probabilistic Models)：
	一种生成模型，主要用于生成高质量的图像。其基本原理是通过逐步的扩散过程，将数据（如图像）逐渐添加噪声，然后学习反向过程，将噪声逐步去除，以恢复原始数据。DDPMs在图像生成、图像修复、超分辨率等领域都有广泛应用。它们是现代生成模型中的一种重要方法，与GANs（生成对抗网络）等方法互为补充。

### CLIP & BLIP：
	两种不同的深度学习模型，主要用于处理图像和文本之间的关系。
	CLIP 使用一种对比损失函数，训练模型通过大量的图像和相应的文本描述来学习图像和文本之间的相互关系。模型的目标是将相关的图像和文本嵌入到相似的特征空间中，而将不相关的嵌入分开。可以用于图像分类、图像检索、文本生成等任务，支持zero-shot学习，即模型能够在未见过的数据上进行有效的推理。
	BLIP 在 CLIP 的基础上，进一步引入了自监督学习和多模态对齐的方法。它通过引导学习图像和文本之间的关系来增强模型对复杂任务的理解能力，例如图像描述生成和视觉问答，适用于更复杂的任务。
	CLIP更侧重于对比学习和零-shot学习的能力，适用于图像和文本的基本匹配任务。BLIP则在此基础上引入更多的自监督学习机制，旨在提高多模态理解和生成能力。

### facial landmarks：
	人脸图像中的关键点或特定位置，用于描述和识别人脸的形状、结构和特征。这些关键点通常是人脸上具有重要意义的位置，如眼睛、眉毛、鼻子、嘴巴、下巴等部位。

### SuTI(Supervised Transformer for Image Generation)：
	一种用于图像生成的深度学习模型。它结合了监督学习和Transformer架构的优势，以提高生成图像的质量和效率。

### Textual Inversion（文本反演）：
	一种文本处理技术，通常用于对文本进行逆向操作或转换，以生成类似原始文本但又具有一定差异的文本输出。这种技术在自然语言处理领域中被广泛使用，可以用于数据增强、对抗性样本生成、文本风格转换等任务。

### ELITE（Ensemble Latent Implicit Transformation Embedding）：
	ELITE 是一种用于图像个性化生成的方法，旨在实现对人脸图像的个性化编辑和生成。ELITE 利用潜在空间嵌入技术，能够在保持人脸身份和特征的同时，实现对图像的个性化编辑，比如改变表情、年龄、姿势等。这种技术结合了潜在变量的概念和生成对抗网络（GAN）的框架，使得用户可以通过操纵潜在空间的特定维度来实现对图像的定制编辑。

## 内容：

### I. INTRODUCTION：

**立意**：最近两年内，生成模型和扩散模型在许多任务，尤其是内容生成任务上发挥了不小的作用。个性化内容合成(PCS)就是一个重要的分支，具有应用前景。从2022年8月起，相关研究论文越来越多，但是缺乏对这些研究的总结工作。因此这篇文章将总结方法、探讨未来的研究方向。

**首要目标**：有效地解决学习过程

**基于训练策略的初步分类：**
- optimization-based 每个个性化任务上都微调一个特定的模型。
- learning-based 训练一个统一的模型解决任何的SoI生成任务。

**限制**：
- 有限的数据资源容易导致过拟合，难以生成用户想要的内容。
- 需要权衡图像对齐和文本保真度。
- 缺乏健壮的评估指标、缺乏标准化的测试数据集、需要更快的处理时间...

**文章特点：**
- 特别关注PCS，而不是对图像合成的概括介绍。
- 将内容个性化分为若干子领域，对于特定任务进行总结。
- 提出现在的挑战和未来的研究方向。

### II. FUNDAMENTALS：

#这部分需要再理解
*text-conditioned diffusion process based on Denoising Diffusion Probabilistic Models (DDPMs)*

*2 base processes : forward and reverse*

### III. GENERIC FRAMEWORK：

#### A. Optimization-based Framework：

##### unique modifer：

- 基于优化框架的一个重要议题是：如何将SoI表达成文本描述形式，使用户可以灵活地生成新的提示。
- **modifer**可以用来符号化SoI。

一个**unique modifer**的构建类型可以分为3类：

	Learnable embedding / Plain text / Rare token

##### Training Prompt Construction：

**原来的问题**：样本的训练提示词最初是添加前缀词(“Photo of V*”)，但这种传统的方式会使训练时间过长，且性能欠佳。

**解决方法**：引入一个类名词，提供高质量的caption。

##### Training Objective：

**训练目标**：为每个特定的个性化任务，训练好一组参数theta。

**常采用的优化选项**：
- token嵌入的优化
- 整个扩散模型的优化
- 参数子集的优化
- 类似adapter，引入新的参数
- LoRA

##### Inference：

- 参数调好之后进入真正的推理阶段，开始个性化图像生成。

#### B. Learning-based Framework：

- 该方法的成功依赖于三个重要因素：
	1. 如何设计一个有效的架构来促进测试时的个性化。
	2. 如何保存SoI的最大信息，以确保视觉保真度。
	3. 要使用的训练数据集的适当大小是多少。

##### Architecture：

基于学习的方法可以分为两大类：
- placeholder based architectures
- reference-conditioned architectures

##### SoI Feature Representation：

- 可使用encoder来提取SoI特征，但是需要排除背景等不相关因素的影响。
- 可融合额外的先验知识来引导学习过程。例如人脸图像中的关键点或特定位置。

##### Training Data：

两种可使用的训练样本模式：
- **Triplet Data** (Reference Image, Target Image, Target Caption).
	**优点**：数据集格式对齐了PCS的目标。
	**缺点**：收集大规模的三联样本有困难。已提出一些解决方法。

- **Dual Data** (Reference Image, Reference Caption).
	**优点**：三联样本的简化版本，更易获取。
	**缺点**：不能很好地适应提示词，难以应对复杂的提示词。


### IV. CATEGORIZATION OF PERSONALIZATION TASKS：

*个性化包括很广泛地范围。例如，目标、风格、人脸等等。*

#### A. Personalized Object Generation

*创造特定目标的定制视觉表达*

##### Textual Inversion：（文本反演）

*在tokenizer中插入一个新的token，以表示SoI。*

- **缺点**：将复杂的视觉特征压缩成一个小集合参数，导致收敛需要较长时间和视觉保真度存在潜在损失。

- **解决方案**：注入的噪声，这导致了传统的收敛指标在确定训练的精确结束方面的失败。在消除所有随机性后，重建损失的信息更加丰富，并设计了一个评估损失方差的停止准则。

##### DreamBooth：

*使用一个upnique modifier来表示SoI，并微调扩散模型的所有参数。*

- **缺点**：微调的模式需要大量的存储空间

- **解决方案**：
	1. 自定义扩散(Custom Diffusion)专注于识别和微调关键参数，特别是交叉注意层中的关键值投影，以实现视觉保真度和存储效率之间的平衡。
	2. 引入注意力矩阵的低秩估计。

##### ELITE：

*结合全局参考特征和文本嵌入，同时结合排除不相关背景的局部特征，专门针对图像个性化。*

- 一些工作已经探索了预训练的多模态大语言模型（MLLM）的使用，它可以在一个统一的框架内处理文本和图像模态。

#### B. Personalized Style Generation：

*为相应的图像搭配特定的美学元素。但是风格一词含义是非常丰富的，包括笔触、材料纹理、配色方案、结构形式、照明技术和文化影响。*

**Different Version:**
	1. StyleDrop
	2. StyleAligned
	3. StyleAdapter

#### C. Personalized Face Generation：

*个性化人脸生成的目标是生成符合文本提示规范的不同身份识别图像，只利用少数初始人脸图像。(讨论范围缩小到特定的类别--人类)*

**问题**：过拟合

**解决方案**：
	1. 在无分类器引导下，通过**融合采样**来平衡伪嵌入和上下文描述预测的噪声。
	2. **二次微调策略** -> 加强保真度。
	3. 个性化人脸可在**预训练的名人人脸扩散模型**的嵌入空间中组合表达。

##### 以下是一些快速发展的learning-based 框架：
1. Face0
	检测和裁剪面部区域，以提取精细的embedding。
2. W+ Adapter
	构建映射网络和残差交叉注意力模块实现脸部特征的空间转换。
3. FaceStudio
	改写交叉注意力模块以支持混合引导。
4. PhotoMaker
	通过一个细致的数据收集和过滤管道构建了一个高质量的数据集。
5. PortraitBooth
	融合文本条件并且缩窄了预训练人脸识别模型的特征。
6. InstantID
	额外引入了一个ControlNet的变体。

#### D. Multiple Subject Composition：

*users have the intention to compose multiple SoI together, which results in the new task, multiple subject composition.*

**挑战**：对于**optimization-based**模型(在特定的SoI任务上微调的)，如何将参数整合到一个模块上。

##### 将多个参数整合到一个统一的参数上：
1. Custom Diffusion
2. Mix-of-Show

##### One-for-one generation following a fusion mechanism：
1. StyleDrop
2. OMG

##### Train a union model on a dataset containing all expected subjects：
1. SVDiff  (Cut-Mix)

Cones：
*神经元保存SoI信息，属于不同SoI的神经元将被同时激活，以生成SoI组合。*

##### 对于learning-based模型：
*由于本身具有多种话题混合的能力，所以可以确保无缝和高效的组合。*

#### E. High-level Semantic Personalization：

*The field of image personalization is expanding to include not just direct visual attributes but also complex semantic relationships and high-level concepts.*

##### Approaches:
1. ReVersion
2. Lego
3. ADI

#### F. Attack and Defense：

*The advancing technologies also present a challenge in terms of potential risky usage.*

*预定义一组触发词和毫无意义的图像。这些数据在训练阶段被成对和合并。一旦遇到触发词，合成的图像将被有意地改变以得到保护。*

#### G. Personalization on Extra Conditions

*Some personalization tasks include additional conditions for content customization.*

#### H. Personalized Video Generation：

*In video personalization, the primary inversion objectives can be categorized into three distinct types: appearance, motion, and the combination of both subject and motion.*

#### I. Personalized 3D generation

#### J. Others

### V. TECHNIQUES IN PERSONALIZED IMAGE SYNTHESIS：

#### A. Attention-based Operation：

*Attention-based operations are a crucial technique in model learning, particularly for processing features effectively.*

e.g.  Query-Key-Value (QKV) scheme

**问题**：unique modifier可能主导attention map，导致只注意SoI而忽略其他细节。

##### 在研究中一些方法已经做出改进：
1. Mix-of-Show替换特征图，用区域感知的交叉注意力来增强语境相关性。
2. DreamTuner采用特定的QKV设计。

##### 限制SoI token对注意力层的影响：
1. Layout-Control：特意调整靠近layout的注意力的权重(无额外训练)。
2. Cones 2：定义一些负注意力区域来惩罚非法占领。
3. VICO：插入一个新的注意层，其中部署了一个二进制掩码。
4. DreamTuner：设计一个可从图像中不同区域高效整合特征的注意力层。

#### B. Mask-guided Generation

*Masks serve as a strong prior that indicates the position and contour of the specified object, which is pivotal for guiding the focus of generative models.*

*许多研究选择丢弃背景像素，使重建损失可以重点关注目标，而不是不相关的干扰物。*

##### Mask可以作为一个监督信号被整合到注意力模块中：
1. Subject-Diffusion：掩蔽了整个扩散阶段的潜在特征。
2. AnyDoor：使用一个额外的高频滤波器提取细节特征与分割的主题作为图像生成过程的条件。
3. DisenBooth：定义一个带有可学习mask的、与id无关的嵌入。
4. PACGen：通过二进制掩码辅助指示主题区域。
5. Face-Diffuser：通过增强由预先训练的文本到图像扩散模型和基于学习的个性化模型预测的噪声来确定mask。

#### C. Data Augmentation：

*Due to limited references, existing methods often struggle to capture complete semantic information of the SoI, resulting in challenges in producing realistic and diverse images. To address this, various techniques employ data augmentation strategies to enrich the diversity of SoI.*

- **COTI：**
	采用评分者网络，从大型网络抓网数据池中选择具有高审美质量的语义相关样本，逐步扩展训练集。
- **SVDiff**：
	手动构建多个SoI的混合图像作为新的训练数据，从而增强了模型在复杂场景下的暴露程度。
- **BLIP-Diffusion**：
	分割前景主题并在随机背景中组成它。
- **DreamIdentity**：
	利用嵌入大规模预训练扩散模型中的名人知识生成源图像和编辑后的人脸图像。
- **PACGen**：
	重新调整规模、中心裁剪和重新定位都可解决时空位置和身份信息纠缠的问题。
- **StyleAdapter**：
	Shuffle the patch，打破不相关的主题，保持想要的风格。
- **Break-A-Scene**:
	对目标主题的随机子集进行抽样，然后采用mask策略。

#### D. Regularization：

*Regularization is a kind of method that is used to regularize the weight update to avoid overfitting or preserve a better appearance.*

1. 使用一个由与SoI具有相同类别的图像组成的附加数据集。
2. 通过重建这些图像，需要个性化的模型来保存预先训练好的知识，这是缓解过拟合问题的有效方法。

e.g.
##### StyleBoost：
- 版本1.0：引入一个用于样式个性化的辅助数据集。
- 版本2.0：引入包括指定形状、背景、颜色和纹理的详细提示，将主题从背景中解耦出来。

##### 利用在大规模数据集上训练的文本前趋：
1. Perfusion
2. Compositional Inversion
3. Cones 2
4. VICO

### VI. EVALUATION：

#### A. Evaluation Dataset：
1. DreamBooth：包括背包、动物、汽车、玩具等30个主题。
2. DreamBench-v2：额外添加了220个测试提示词。
3. Custom Diffusion：10个主题，每个主题有20条特定的测试提示词。
4. Custom-101：101个主题，提供更广范围的评估。
5. Stellar：特别针对以人为中心的评估，对400个人类身份有2万条提示词。

#### B. Evaluation Metrics：

*The metrics are designed for text alignment and visual similarity.*

e.g.
CLIP similarity score of text features and image features

##### Conventional metrics：
1. Fr´echet Inception Distance (FID)
2. Inception Score (IS)

##### Specific metrics for human personalization：
1. soft-penalized CLIP
2. text score
3. Identity Preservation Score
4. Attribute Preservation Score
5. Stability of Identity Score
6. Grounding Objects Accuracy
7. Relation Fidelity Score

### VII. CHALLENGE AND OUTLOOK：

#### A. Overfitting Problem：

**体现**：
1. 缺乏SoI可编辑性。
2. 包含不相关的语义。

**问题根源**：
1. 与预先训练好的单词形成的中心分布相比，学习到的标记嵌入位于分布外区域。
2. 伪标记嵌入明显偏离了初始嵌入的分布。
3. 与其他上下文token相比，unique modifier在交叉注意层中占主导地位。

**解决方案**：排除冗余背景、注意力操纵、可学习参数的正则化和数据增强。不过该问题目前为止还没有很好地解决。

#### B. Trade-off on Subject Fidelity and Text Alignment：

**Conflict**：fidelity <-> text alignment

**解决方案**：
1. Perfusion：规范化注意力投影。
2. 将条件指导解耦成两个独立的过程，分别处理两个问题。
仍然有探索和进步的空间。


#### C. Standardization and Evaluation：

*缺乏标准化的测试数据集和健壮的评估指标来衡量不同方法。*

### VIII. CONCLUSION.
